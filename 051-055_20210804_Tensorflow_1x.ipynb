{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"051-055_20210804_Tensorflow_1x.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyMWM3+isz2XoP0vCQD9YTnQ"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"e9_lz46WO_SP","executionInfo":{"status":"ok","timestamp":1628048889289,"user_tz":-540,"elapsed":51989,"user":{"displayName":"SeokHwan Yang","photoUrl":"","userId":"04573916107899292925"}},"outputId":"3c2385e9-3b78-4ed9-9fed-aa0905702730"},"source":["# !pip uninstall tensorflow"],"execution_count":2,"outputs":[{"output_type":"stream","text":["Found existing installation: tensorflow 2.5.0\n","Uninstalling tensorflow-2.5.0:\n","  Would remove:\n","    /usr/local/bin/estimator_ckpt_converter\n","    /usr/local/bin/import_pb_to_tensorboard\n","    /usr/local/bin/saved_model_cli\n","    /usr/local/bin/tensorboard\n","    /usr/local/bin/tf_upgrade_v2\n","    /usr/local/bin/tflite_convert\n","    /usr/local/bin/toco\n","    /usr/local/bin/toco_from_protos\n","    /usr/local/lib/python3.7/dist-packages/tensorflow-2.5.0.dist-info/*\n","    /usr/local/lib/python3.7/dist-packages/tensorflow/*\n","Proceed (y/n)? y\n","  Successfully uninstalled tensorflow-2.5.0\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"vsPp9eeiPN13","executionInfo":{"status":"ok","timestamp":1628048975083,"user_tz":-540,"elapsed":55444,"user":{"displayName":"SeokHwan Yang","photoUrl":"","userId":"04573916107899292925"}},"outputId":"b8e36b04-39bb-4f69-db42-e8fa000cae13"},"source":["# !pip install tensorflow==1.15"],"execution_count":3,"outputs":[{"output_type":"stream","text":["Collecting tensorflow==1.15\n","  Downloading tensorflow-1.15.0-cp37-cp37m-manylinux2010_x86_64.whl (412.3 MB)\n","\u001b[K     |████████████████████████████████| 412.3 MB 23 kB/s \n","\u001b[?25hCollecting keras-applications>=1.0.8\n","  Downloading Keras_Applications-1.0.8-py3-none-any.whl (50 kB)\n","\u001b[K     |████████████████████████████████| 50 kB 6.1 MB/s \n","\u001b[?25hRequirement already satisfied: protobuf>=3.6.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15) (3.17.3)\n","Requirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15) (1.12.1)\n","Collecting tensorflow-estimator==1.15.1\n","  Downloading tensorflow_estimator-1.15.1-py2.py3-none-any.whl (503 kB)\n","\u001b[K     |████████████████████████████████| 503 kB 60.6 MB/s \n","\u001b[?25hRequirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15) (1.15.0)\n","Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15) (1.34.1)\n","Requirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15) (0.12.0)\n","Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15) (1.1.2)\n","Requirement already satisfied: astor>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15) (0.8.1)\n","Requirement already satisfied: numpy<2.0,>=1.16.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15) (1.19.5)\n","Collecting gast==0.2.2\n","  Downloading gast-0.2.2.tar.gz (10 kB)\n","Collecting tensorboard<1.16.0,>=1.15.0\n","  Downloading tensorboard-1.15.0-py3-none-any.whl (3.8 MB)\n","\u001b[K     |████████████████████████████████| 3.8 MB 24.7 MB/s \n","\u001b[?25hRequirement already satisfied: google-pasta>=0.1.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15) (0.2.0)\n","Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15) (0.36.2)\n","Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15) (1.1.0)\n","Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15) (3.3.0)\n","Requirement already satisfied: h5py in /usr/local/lib/python3.7/dist-packages (from keras-applications>=1.0.8->tensorflow==1.15) (3.1.0)\n","Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<1.16.0,>=1.15.0->tensorflow==1.15) (57.2.0)\n","Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard<1.16.0,>=1.15.0->tensorflow==1.15) (3.3.4)\n","Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard<1.16.0,>=1.15.0->tensorflow==1.15) (1.0.1)\n","Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard<1.16.0,>=1.15.0->tensorflow==1.15) (4.6.1)\n","Requirement already satisfied: cached-property in /usr/local/lib/python3.7/dist-packages (from h5py->keras-applications>=1.0.8->tensorflow==1.15) (1.5.2)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->markdown>=2.6.8->tensorboard<1.16.0,>=1.15.0->tensorflow==1.15) (3.5.0)\n","Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->markdown>=2.6.8->tensorboard<1.16.0,>=1.15.0->tensorflow==1.15) (3.7.4.3)\n","Building wheels for collected packages: gast\n","  Building wheel for gast (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for gast: filename=gast-0.2.2-py3-none-any.whl size=7553 sha256=e0395aadc71c06a2b0f40b542e0a23d7472eb3dbd63c6168bab050b5c766cb67\n","  Stored in directory: /root/.cache/pip/wheels/21/7f/02/420f32a803f7d0967b48dd823da3f558c5166991bfd204eef3\n","Successfully built gast\n","Installing collected packages: tensorflow-estimator, tensorboard, keras-applications, gast, tensorflow\n","  Attempting uninstall: tensorflow-estimator\n","    Found existing installation: tensorflow-estimator 2.5.0\n","    Uninstalling tensorflow-estimator-2.5.0:\n","      Successfully uninstalled tensorflow-estimator-2.5.0\n","  Attempting uninstall: tensorboard\n","    Found existing installation: tensorboard 2.5.0\n","    Uninstalling tensorboard-2.5.0:\n","      Successfully uninstalled tensorboard-2.5.0\n","  Attempting uninstall: gast\n","    Found existing installation: gast 0.4.0\n","    Uninstalling gast-0.4.0:\n","      Successfully uninstalled gast-0.4.0\n","\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","tensorflow-probability 0.13.0 requires gast>=0.3.2, but you have gast 0.2.2 which is incompatible.\n","kapre 0.3.5 requires tensorflow>=2.0.0, but you have tensorflow 1.15.0 which is incompatible.\u001b[0m\n","Successfully installed gast-0.2.2 keras-applications-1.0.8 tensorboard-1.15.0 tensorflow-1.15.0 tensorflow-estimator-1.15.1\n"],"name":"stdout"},{"output_type":"display_data","data":{"application/vnd.colab-display-data+json":{"pip_warning":{"packages":["gast","tensorboard","tensorflow"]}}},"metadata":{"tags":[]}}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"KGFN50x3Mq1O","executionInfo":{"status":"ok","timestamp":1628049191405,"user_tz":-540,"elapsed":1999,"user":{"displayName":"SeokHwan Yang","photoUrl":"","userId":"04573916107899292925"}},"outputId":"cccceab2-312a-4930-8596-b50d2cc2bba8"},"source":["import tensorflow as tf\n","\n","a = tf.constant([5],dtype=tf.float32)\n","b = tf.constant([10],dtype=tf.float32)\n","c = tf.constant([2],dtype=tf.float32)\n","\n","d = a*b+c\n","\n","print(d)\n","\n","sess = tf.Session()\n","result = sess.run(d)\n","print (result)"],"execution_count":1,"outputs":[{"output_type":"stream","text":["Tensor(\"add:0\", shape=(1,), dtype=float32)\n","[52.]\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"fJbzfGcMNJpq","executionInfo":{"status":"ok","timestamp":1628049492434,"user_tz":-540,"elapsed":328,"user":{"displayName":"SeokHwan Yang","photoUrl":"","userId":"04573916107899292925"}},"outputId":"0ec58b35-e326-4735-b673-0089e4e4f170"},"source":["import tensorflow as tf\n","\n","# 상수 노드 정의\n","a = tf.constant(1.0)\n","b = tf.constant(2.0)\n","c = tf.constant([ [1.0, 2.0], [3.0, 4.0] ])\n","\n","print(a)\n","print(a+b)\n","print(c)\n","\n","# 세션 (session) 을 만들고 노드간의 텐서 연산 실행\n","sess = tf.Session()\n","print(sess.run([a, b]))\n","print(sess.run(c))\n","print(sess.run([a+b]))\n","print(sess.run(c+1.0)) # broadcast 수행\n","\n","# 세션 close\n","sess.close() "],"execution_count":3,"outputs":[{"output_type":"stream","text":["Tensor(\"Const_3:0\", shape=(), dtype=float32)\n","Tensor(\"add_1:0\", shape=(), dtype=float32)\n","Tensor(\"Const_5:0\", shape=(2, 2), dtype=float32)\n","[1.0, 2.0]\n","[[1. 2.]\n"," [3. 4.]]\n","[3.0]\n","[[2. 3.]\n"," [4. 5.]]\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"nnZyniDbNnuY","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1628050663766,"user_tz":-540,"elapsed":298,"user":{"displayName":"SeokHwan Yang","photoUrl":"","userId":"04573916107899292925"}},"outputId":"224fc72c-3418-4b82-c3ba-350d8d2fcdbd"},"source":["import tensorflow as tf\n","\n","input_data = [1,2,3,4,5]\n","x = tf.placeholder(dtype=tf.float32)\n","y = x * 2\n","\n","sess = tf.Session()\n","result = sess.run(y,feed_dict={x:input_data})\n","\n","print(result)"],"execution_count":4,"outputs":[{"output_type":"stream","text":["[ 2.  4.  6.  8. 10.]\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"gEhLlR1CYaZd","executionInfo":{"status":"error","timestamp":1628051296639,"user_tz":-540,"elapsed":2595,"user":{"displayName":"SeokHwan Yang","photoUrl":"","userId":"04573916107899292925"}},"outputId":"018d5567-7e1e-47d1-d901-24433adc2497"},"source":["import tensorflow as tf\n","\n","input_data = [1,2,3,4,5]\n","x = tf.placeholder(dtype=tf.float32)\n","W = tf.Variable([2],dtype=tf.float32)\n","y = W*x\n","\n","sess = tf.Session()\n","result = sess.run(y,feed_dict={x:input_data})\n","\n","print(result)"],"execution_count":5,"outputs":[{"output_type":"error","ename":"FailedPreconditionError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mFailedPreconditionError\u001b[0m                   Traceback (most recent call last)","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow_core/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1364\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1365\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1366\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow_core/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1349\u001b[0m       return self._call_tf_sessionrun(options, feed_dict, fetch_list,\n\u001b[0;32m-> 1350\u001b[0;31m                                       target_list, run_metadata)\n\u001b[0m\u001b[1;32m   1351\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow_core/python/client/session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[0;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[1;32m   1442\u001b[0m                                             \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1443\u001b[0;31m                                             run_metadata)\n\u001b[0m\u001b[1;32m   1444\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mFailedPreconditionError\u001b[0m: Attempting to use uninitialized value Variable\n\t [[{{node Variable/read}}]]","\nDuring handling of the above exception, another exception occurred:\n","\u001b[0;31mFailedPreconditionError\u001b[0m                   Traceback (most recent call last)","\u001b[0;32m<ipython-input-5-25a6421a0c78>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0msess\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSession\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0minput_data\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow_core/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    954\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    955\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 956\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    957\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    958\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow_core/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1178\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1179\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1180\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1181\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1182\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow_core/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1357\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1358\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[0;32m-> 1359\u001b[0;31m                            run_metadata)\n\u001b[0m\u001b[1;32m   1360\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1361\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow_core/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1382\u001b[0m                     \u001b[0;34m'\\nsession_config.graph_options.rewrite_options.'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1383\u001b[0m                     'disable_meta_optimizer = True')\n\u001b[0;32m-> 1384\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnode_def\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1385\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1386\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mFailedPreconditionError\u001b[0m: Attempting to use uninitialized value Variable\n\t [[node Variable/read (defined at /usr/local/lib/python3.7/dist-packages/tensorflow_core/python/framework/ops.py:1748) ]]\n\nOriginal stack trace for 'Variable/read':\n  File \"/usr/lib/python3.7/runpy.py\", line 193, in _run_module_as_main\n    \"__main__\", mod_spec)\n  File \"/usr/lib/python3.7/runpy.py\", line 85, in _run_code\n    exec(code, run_globals)\n  File \"/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py\", line 16, in <module>\n    app.launch_new_instance()\n  File \"/usr/local/lib/python3.7/dist-packages/traitlets/config/application.py\", line 845, in launch_instance\n    app.start()\n  File \"/usr/local/lib/python3.7/dist-packages/ipykernel/kernelapp.py\", line 499, in start\n    self.io_loop.start()\n  File \"/usr/local/lib/python3.7/dist-packages/tornado/platform/asyncio.py\", line 132, in start\n    self.asyncio_loop.run_forever()\n  File \"/usr/lib/python3.7/asyncio/base_events.py\", line 541, in run_forever\n    self._run_once()\n  File \"/usr/lib/python3.7/asyncio/base_events.py\", line 1786, in _run_once\n    handle._run()\n  File \"/usr/lib/python3.7/asyncio/events.py\", line 88, in _run\n    self._context.run(self._callback, *self._args)\n  File \"/usr/local/lib/python3.7/dist-packages/tornado/platform/asyncio.py\", line 122, in _handle_events\n    handler_func(fileobj, events)\n  File \"/usr/local/lib/python3.7/dist-packages/tornado/stack_context.py\", line 300, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/usr/local/lib/python3.7/dist-packages/zmq/eventloop/zmqstream.py\", line 451, in _handle_events\n    self._handle_recv()\n  File \"/usr/local/lib/python3.7/dist-packages/zmq/eventloop/zmqstream.py\", line 480, in _handle_recv\n    self._run_callback(callback, msg)\n  File \"/usr/local/lib/python3.7/dist-packages/zmq/eventloop/zmqstream.py\", line 434, in _run_callback\n    callback(*args, **kwargs)\n  File \"/usr/local/lib/python3.7/dist-packages/tornado/stack_context.py\", line 300, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/usr/local/lib/python3.7/dist-packages/ipykernel/kernelbase.py\", line 283, in dispatcher\n    return self.dispatch_shell(stream, msg)\n  File \"/usr/local/lib/python3.7/dist-packages/ipykernel/kernelbase.py\", line 233, in dispatch_shell\n    handler(stream, idents, msg)\n  File \"/usr/local/lib/python3.7/dist-packages/ipykernel/kernelbase.py\", line 399, in execute_request\n    user_expressions, allow_stdin)\n  File \"/usr/local/lib/python3.7/dist-packages/ipykernel/ipkernel.py\", line 208, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"/usr/local/lib/python3.7/dist-packages/ipykernel/zmqshell.py\", line 537, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"/usr/local/lib/python3.7/dist-packages/IPython/core/interactiveshell.py\", line 2718, in run_cell\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"/usr/local/lib/python3.7/dist-packages/IPython/core/interactiveshell.py\", line 2822, in run_ast_nodes\n    if self.run_code(code, result):\n  File \"/usr/local/lib/python3.7/dist-packages/IPython/core/interactiveshell.py\", line 2882, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-5-25a6421a0c78>\", line 5, in <module>\n    W = tf.Variable([2],dtype=tf.float32)\n  File \"/usr/local/lib/python3.7/dist-packages/tensorflow_core/python/ops/variables.py\", line 258, in __call__\n    return cls._variable_v1_call(*args, **kwargs)\n  File \"/usr/local/lib/python3.7/dist-packages/tensorflow_core/python/ops/variables.py\", line 219, in _variable_v1_call\n    shape=shape)\n  File \"/usr/local/lib/python3.7/dist-packages/tensorflow_core/python/ops/variables.py\", line 197, in <lambda>\n    previous_getter = lambda **kwargs: default_variable_creator(None, **kwargs)\n  File \"/usr/local/lib/python3.7/dist-packages/tensorflow_core/python/ops/variable_scope.py\", line 2519, in default_variable_creator\n    shape=shape)\n  File \"/usr/local/lib/python3.7/dist-packages/tensorflow_core/python/ops/variables.py\", line 262, in __call__\n    return super(VariableMetaclass, cls).__call__(*args, **kwargs)\n  File \"/usr/local/lib/python3.7/dist-packages/tensorflow_core/python/ops/variables.py\", line 1688, in __init__\n    shape=shape)\n  File \"/usr/local/lib/python3.7/dist-packages/tensorflow_core/python/ops/variables.py\", line 1872, in _init_from_args\n    self._snapshot = array_ops.identity(self._variable, name=\"read\")\n  File \"/usr/local/lib/python3.7/dist-packages/tensorflow_core/python/util/dispatch.py\", line 180, in wrapper\n    return target(*args, **kwargs)\n  File \"/usr/local/lib/python3.7/dist-packages/tensorflow_core/python/ops/array_ops.py\", line 203, in identity\n    ret = gen_array_ops.identity(input, name=name)\n  File \"/usr/local/lib/python3.7/dist-packages/tensorflow_core/python/ops/gen_array_ops.py\", line 4239, in identity\n    \"Identity\", input=input, name=name)\n  File \"/usr/local/lib/python3.7/dist-packages/tensorflow_core/python/framework/op_def_library.py\", line 794, in _apply_op_helper\n    op_def=op_def)\n  File \"/usr/local/lib/python3.7/dist-packages/tensorflow_core/python/util/deprecation.py\", line 507, in new_func\n    return func(*args, **kwargs)\n  File \"/usr/local/lib/python3.7/dist-packages/tensorflow_core/python/framework/ops.py\", line 3357, in create_op\n    attrs, op_def, compute_device)\n  File \"/usr/local/lib/python3.7/dist-packages/tensorflow_core/python/framework/ops.py\", line 3426, in _create_op_internal\n    op_def=op_def)\n  File \"/usr/local/lib/python3.7/dist-packages/tensorflow_core/python/framework/ops.py\", line 1748, in __init__\n    self._traceback = tf_stack.extract_stack()\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"A8QXcaVSa0WB","executionInfo":{"status":"ok","timestamp":1628051700501,"user_tz":-540,"elapsed":426,"user":{"displayName":"SeokHwan Yang","photoUrl":"","userId":"04573916107899292925"}},"outputId":"5f4e3be7-a189-4f72-863e-c779d23e27e6"},"source":["import tensorflow as tf\n","\n","input_data = [1,2,3,4,5]\n","x = tf.placeholder(dtype=tf.float32)\n","W = tf.Variable([2],dtype=tf.float32)\n","y = W*x\n","\n","sess = tf.Session()\n","init = tf.global_variables_initializer()\n","sess.run(init)\n","result = sess.run(y,feed_dict={x:input_data})\n","\n","print(result)"],"execution_count":6,"outputs":[{"output_type":"stream","text":["[ 2.  4.  6.  8. 10.]\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"z2dVTd51cXeP","executionInfo":{"status":"ok","timestamp":1628052208561,"user_tz":-540,"elapsed":309,"user":{"displayName":"SeokHwan Yang","photoUrl":"","userId":"04573916107899292925"}},"outputId":"5ddc200d-9866-4db5-fc4c-fdd02b7dcffd"},"source":["import tensorflow as tf\n","\n","x = tf.constant([ [1.0,2.0,3.0] ])\n","w = tf.constant([ [2.0],[2.0],[2.0] ])\n","y = tf.matmul(x,w)\n","print(x.get_shape())\n","\n","sess = tf.Session()\n","init = tf.global_variables_initializer()\n","sess.run(init)\n","result = sess.run(y)\n","\n","print(result)"],"execution_count":8,"outputs":[{"output_type":"stream","text":["(1, 3)\n","[[12.]]\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"yCZEa6doeRGf","executionInfo":{"status":"ok","timestamp":1628052420844,"user_tz":-540,"elapsed":312,"user":{"displayName":"SeokHwan Yang","photoUrl":"","userId":"04573916107899292925"}},"outputId":"3df25138-4b4c-41ce-f76a-5124ded3459b"},"source":["import tensorflow as tf\n","\n","x = tf.Variable([ [1.,2.,3.] ], dtype=tf.float32)\n","w = tf.constant([ [2.],[2.],[2.]], dtype=tf.float32)\n","y = tf.matmul(x,w)\n","\n","sess = tf.Session()\n","init = tf.global_variables_initializer()\n","sess.run(init)\n","result = sess.run(y)\n","\n","print(result)"],"execution_count":9,"outputs":[{"output_type":"stream","text":["[[12.]]\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"wNOrKt4_fHXN","executionInfo":{"status":"ok","timestamp":1628052634299,"user_tz":-540,"elapsed":319,"user":{"displayName":"SeokHwan Yang","photoUrl":"","userId":"04573916107899292925"}},"outputId":"f88ddd4d-2aa1-44bb-ee66-bcb8fe7ed303"},"source":["import tensorflow as tf\n","\n","input_data = [ [1.,2.,3.],[1.,2.,3.],[2.,3.,4.] ] #3x3 matrix\n","x = tf.placeholder(dtype=tf.float32,shape=[None,3])\n","w = tf.Variable([ [2.],[2.],[2.] ], dtype = tf.float32) #3x1 matrix\n","y = tf.matmul(x,w)\n","\n","sess = tf.Session()\n","init = tf.global_variables_initializer()\n","sess.run(init)\n","result = sess.run(y,feed_dict={x:input_data})\n","\n","print(result)"],"execution_count":10,"outputs":[{"output_type":"stream","text":["[[12.]\n"," [12.]\n"," [18.]]\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"wynh641lf7eP","executionInfo":{"status":"ok","timestamp":1628053101124,"user_tz":-540,"elapsed":336,"user":{"displayName":"SeokHwan Yang","photoUrl":"","userId":"04573916107899292925"}},"outputId":"f32fc282-bd62-4e1c-9224-7d3c6af0d643"},"source":["import tensorflow as tf\n","\n","input_data = [\n","     [1,1,1],[2,2,2]\n","    ]\n","x = tf.placeholder(dtype=tf.float32,shape=[2,3])\n","w  =tf.Variable([[2],[2],[2]],dtype=tf.float32)\n","b  =tf.Variable([4],dtype=tf.float32)\n","y = tf.matmul(x,w)+b\n","\n","print(x.get_shape())\n","\n","sess = tf.Session()\n","init = tf.global_variables_initializer()\n","sess.run(init)\n","result = sess.run(y,feed_dict={x:input_data})\n","\n","print(result)"],"execution_count":11,"outputs":[{"output_type":"stream","text":["(2, 3)\n","[[10.]\n"," [16.]]\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"B7EDj8ZChtcW","executionInfo":{"status":"ok","timestamp":1628055039615,"user_tz":-540,"elapsed":32569,"user":{"displayName":"SeokHwan Yang","photoUrl":"","userId":"04573916107899292925"}},"outputId":"1b542de8-0f9f-449a-bf5a-b6e3a8543bc3"},"source":["from google.colab import drive\n","drive.mount('/gdrive', force_remount=True)"],"execution_count":12,"outputs":[{"output_type":"stream","text":["Mounted at /gdrive\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"nMRp6oFco-rZ","executionInfo":{"status":"ok","timestamp":1628055043906,"user_tz":-540,"elapsed":307,"user":{"displayName":"SeokHwan Yang","photoUrl":"","userId":"04573916107899292925"}}},"source":["ROOT_PATH = '/gdrive/My Drive/Colab Notebooks/Lectures/'"],"execution_count":13,"outputs":[]},{"cell_type":"code","metadata":{"id":"jkKLPP7cq4G5","executionInfo":{"status":"ok","timestamp":1628055517921,"user_tz":-540,"elapsed":299,"user":{"displayName":"SeokHwan Yang","photoUrl":"","userId":"04573916107899292925"}}},"source":["from tensorflow.examples.tutorials.mnist import input_data\n","import tensorflow as tf\n","import numpy as np"],"execution_count":19,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"YiIF2ZrcpHw5","executionInfo":{"status":"ok","timestamp":1628055279370,"user_tz":-540,"elapsed":2046,"user":{"displayName":"SeokHwan Yang","photoUrl":"","userId":"04573916107899292925"}},"outputId":"9c87d870-58ab-48bf-a7c5-33f9284df5e3"},"source":["filepath = '{}/{}/{}/'.format(ROOT_PATH, 'data', 'mnist')\n","mnist = input_data.read_data_sets(filepath, one_hot=True)"],"execution_count":16,"outputs":[{"output_type":"stream","text":["Successfully downloaded train-images-idx3-ubyte.gz 9912422 bytes.\n","WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/tensorflow_core/contrib/learn/python/learn/datasets/mnist.py:262: extract_images (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Please use tf.data to implement this functionality.\n","Extracting /gdrive/My Drive/Colab Notebooks/Lectures//data/mnist/train-images-idx3-ubyte.gz\n","Successfully downloaded train-labels-idx1-ubyte.gz 28881 bytes.\n","WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/tensorflow_core/contrib/learn/python/learn/datasets/mnist.py:267: extract_labels (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Please use tf.data to implement this functionality.\n","Extracting /gdrive/My Drive/Colab Notebooks/Lectures//data/mnist/train-labels-idx1-ubyte.gz\n","WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/tensorflow_core/contrib/learn/python/learn/datasets/mnist.py:110: dense_to_one_hot (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Please use tf.one_hot on tensors.\n","Successfully downloaded t10k-images-idx3-ubyte.gz 1648877 bytes.\n","Extracting /gdrive/My Drive/Colab Notebooks/Lectures//data/mnist/t10k-images-idx3-ubyte.gz\n","Successfully downloaded t10k-labels-idx1-ubyte.gz 4542 bytes.\n","Extracting /gdrive/My Drive/Colab Notebooks/Lectures//data/mnist/t10k-labels-idx1-ubyte.gz\n","WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/tensorflow_core/contrib/learn/python/learn/datasets/mnist.py:290: DataSet.__init__ (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"O8LN_XBYqzmH","executionInfo":{"status":"ok","timestamp":1628055523572,"user_tz":-540,"elapsed":303,"user":{"displayName":"SeokHwan Yang","photoUrl":"","userId":"04573916107899292925"}},"outputId":"603507cd-1937-427b-9de2-9624ed1c3395"},"source":["print(\"\\ntrain image shape = \", np.shape(mnist.train.images))\n","print(\"train label shape = \", np.shape(mnist.train.labels))\n","print(\"test image shape = \", np.shape(mnist.test.images))\n","print(\"test label shape = \", np.shape(mnist.test.labels))"],"execution_count":20,"outputs":[{"output_type":"stream","text":["\n","train image shape =  (55000, 784)\n","train label shape =  (55000, 10)\n","test image shape =  (10000, 784)\n","test label shape =  (10000, 10)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"oM26XkXopnvF","executionInfo":{"status":"ok","timestamp":1628056078826,"user_tz":-540,"elapsed":149039,"user":{"displayName":"SeokHwan Yang","photoUrl":"","userId":"04573916107899292925"}},"outputId":"1c4f29e4-bdb8-4147-c630-251a236153ca"},"source":["# 신경망 구조 및 하이퍼 파라미터 설정\n","learning_rate = 0.1 # 학습율\n","epochs = 100 # 반복횟수\n","batch_size = 100 # 한번에 입력으로 주어지는 MNIST 데이터 개수\n","input_nodes = 784 # 입력 층 노드 개수\n","hidden_nodes = 100 # 은닉 층 노드 개수\n","output_nodes = 10 # 출력 층 노드 개수\n","\n","# 입력과 출력을 위한 플레이스홀더(placeholder) 정의\n","X = tf.placeholder(tf.float32, [None, input_nodes])\n","T = tf.placeholder(tf.float32, [None, output_nodes])\n","\n","# 가중치와 바이어스 노드 정의\n","W2 = tf.Variable(tf.random_normal([input_nodes, hidden_nodes]))\n","b2 = tf.Variable(tf.random_normal([hidden_nodes]))\n","W3 = tf.Variable(tf.random_normal([hidden_nodes, output_nodes]))\n","b3 = tf.Variable(tf.random_normal([output_nodes]))\n","\n","# 피드포워드 수행 노드 정의\n","Z2 = tf.matmul(X, W2) + b2 # 은닉층 선형회귀 값 Z2\n","A2 = tf.nn.relu(Z2) # 은닉층 출력 값 A2\n","Z3 = logits = tf.matmul(A2, W3) + b3 # 출력층 선형회귀 값 Z3\n","y = A3 = tf.nn.softmax(Z3) # 출력층 출력 값 A3\n","\n","# 손실함수 계산 및 가중치, 바이어스 업데이트\n","cross_entropy = tf.nn.softmax_cross_entropy_with_logits_v2(logits=Z3, labels=T)\n","loss = tf.reduce_mean(cross_entropy)\n","optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n","train = optimizer.minimize(loss)\n","\n","# 정확도 검증 노드\n","predicted_val = tf.equal( tf.argmax(A3, 1), tf.argmax(T, 1) )\n","accuracy = tf.reduce_mean(tf.cast(predicted_val, dtype=tf.float32))\n","\n","# MNIST 인식 정확도 검증\n","with tf.Session() as sess:\n","\tsess.run(tf.global_variables_initializer())\n","\n","\tfor I in range(epochs):\n","\t\ttotal_batch = int(mnist.train.num_examples / batch_size) # 55,000 / 100\n","\n","\t\tfor step in range(total_batch):\n","\t\t\tbatch_x, batch_t = mnist.train.next_batch(batch_size)\n","\t\t\tloss_val, _ = sess.run([loss, train], feed_dict={X: batch_x, T: batch_t})\n","\n","\t\t\tif step % 100 == 0:\n","\t\t\t\tprint('step = ', step, ', loss_val = ', loss_val)\n","\n","\t\t# Accuracy 확인\n","\t\ttest_x_data = mnist.test.images\n","\t\ttest_t_data = mnist.test.labels\n","\t\taccuracy_val = sess.run(accuracy, feed_dict={X: test_x_data, T: test_t_data})\n","\t\tprint(\"\\nAccuracy = \", accuracy_val)"],"execution_count":24,"outputs":[{"output_type":"stream","text":["step =  0 , loss_val =  102.172585\n","step =  100 , loss_val =  3.9716425\n","step =  200 , loss_val =  3.0084152\n","step =  300 , loss_val =  2.6459744\n","step =  400 , loss_val =  1.0566043\n","step =  500 , loss_val =  2.128511\n","\n","Accuracy =  0.8597\n","step =  0 , loss_val =  0.39965266\n","step =  100 , loss_val =  1.6609597\n","step =  200 , loss_val =  0.46147424\n","step =  300 , loss_val =  0.85154253\n","step =  400 , loss_val =  1.1934104\n","step =  500 , loss_val =  0.85407317\n","\n","Accuracy =  0.8693\n","step =  0 , loss_val =  0.9349872\n","step =  100 , loss_val =  0.6614941\n","step =  200 , loss_val =  0.67386603\n","step =  300 , loss_val =  0.5770121\n","step =  400 , loss_val =  0.535161\n","step =  500 , loss_val =  1.1416191\n","\n","Accuracy =  0.8736\n","step =  0 , loss_val =  0.42488164\n","step =  100 , loss_val =  1.0373819\n","step =  200 , loss_val =  0.5586882\n","step =  300 , loss_val =  0.41506532\n","step =  400 , loss_val =  0.6209833\n","step =  500 , loss_val =  0.5755485\n","\n","Accuracy =  0.8793\n","step =  0 , loss_val =  0.7186689\n","step =  100 , loss_val =  0.40348545\n","step =  200 , loss_val =  0.3101944\n","step =  300 , loss_val =  0.33114582\n","step =  400 , loss_val =  0.35733682\n","step =  500 , loss_val =  0.42046508\n","\n","Accuracy =  0.8808\n","step =  0 , loss_val =  0.7082952\n","step =  100 , loss_val =  0.61679494\n","step =  200 , loss_val =  0.22995865\n","step =  300 , loss_val =  0.3947708\n","step =  400 , loss_val =  0.38401765\n","step =  500 , loss_val =  0.4765126\n","\n","Accuracy =  0.89\n","step =  0 , loss_val =  0.59511137\n","step =  100 , loss_val =  0.1557835\n","step =  200 , loss_val =  0.5364422\n","step =  300 , loss_val =  0.24307705\n","step =  400 , loss_val =  0.41387868\n","step =  500 , loss_val =  0.35049996\n","\n","Accuracy =  0.8895\n","step =  0 , loss_val =  0.3306961\n","step =  100 , loss_val =  0.35104987\n","step =  200 , loss_val =  0.4010235\n","step =  300 , loss_val =  0.19125013\n","step =  400 , loss_val =  0.22313969\n","step =  500 , loss_val =  0.32696924\n","\n","Accuracy =  0.8894\n","step =  0 , loss_val =  0.46895257\n","step =  100 , loss_val =  0.4085331\n","step =  200 , loss_val =  0.32649907\n","step =  300 , loss_val =  0.46749976\n","step =  400 , loss_val =  0.39561448\n","step =  500 , loss_val =  0.17876782\n","\n","Accuracy =  0.9022\n","step =  0 , loss_val =  0.2115956\n","step =  100 , loss_val =  0.35392135\n","step =  200 , loss_val =  0.21984735\n","step =  300 , loss_val =  0.33918914\n","step =  400 , loss_val =  0.37870818\n","step =  500 , loss_val =  0.16381986\n","\n","Accuracy =  0.908\n","step =  0 , loss_val =  0.36289015\n","step =  100 , loss_val =  0.61939013\n","step =  200 , loss_val =  0.18581751\n","step =  300 , loss_val =  0.3223373\n","step =  400 , loss_val =  0.23673396\n","step =  500 , loss_val =  0.557998\n","\n","Accuracy =  0.9105\n","step =  0 , loss_val =  0.3053701\n","step =  100 , loss_val =  0.19767389\n","step =  200 , loss_val =  0.50709414\n","step =  300 , loss_val =  0.36447033\n","step =  400 , loss_val =  0.19346292\n","step =  500 , loss_val =  0.23100467\n","\n","Accuracy =  0.9134\n","step =  0 , loss_val =  0.24663113\n","step =  100 , loss_val =  0.3032364\n","step =  200 , loss_val =  0.19662744\n","step =  300 , loss_val =  0.111323245\n","step =  400 , loss_val =  0.28857478\n","step =  500 , loss_val =  0.14932625\n","\n","Accuracy =  0.9104\n","step =  0 , loss_val =  0.20916852\n","step =  100 , loss_val =  0.21987402\n","step =  200 , loss_val =  0.18025765\n","step =  300 , loss_val =  0.26712063\n","step =  400 , loss_val =  0.2360619\n","step =  500 , loss_val =  0.31858575\n","\n","Accuracy =  0.9004\n","step =  0 , loss_val =  0.27245873\n","step =  100 , loss_val =  0.24700771\n","step =  200 , loss_val =  0.12360841\n","step =  300 , loss_val =  0.28239906\n","step =  400 , loss_val =  0.24826942\n","step =  500 , loss_val =  0.19705603\n","\n","Accuracy =  0.9149\n","step =  0 , loss_val =  0.18908864\n","step =  100 , loss_val =  0.16349798\n","step =  200 , loss_val =  0.36943436\n","step =  300 , loss_val =  0.10214834\n","step =  400 , loss_val =  0.24525158\n","step =  500 , loss_val =  0.11322209\n","\n","Accuracy =  0.9147\n","step =  0 , loss_val =  0.18471856\n","step =  100 , loss_val =  0.18088146\n","step =  200 , loss_val =  0.24198593\n","step =  300 , loss_val =  0.2067526\n","step =  400 , loss_val =  0.3314054\n","step =  500 , loss_val =  0.21483436\n","\n","Accuracy =  0.9132\n","step =  0 , loss_val =  0.2193222\n","step =  100 , loss_val =  0.19257696\n","step =  200 , loss_val =  0.2601906\n","step =  300 , loss_val =  0.30507916\n","step =  400 , loss_val =  0.39787453\n","step =  500 , loss_val =  0.24615574\n","\n","Accuracy =  0.9232\n","step =  0 , loss_val =  0.2518821\n","step =  100 , loss_val =  0.34443223\n","step =  200 , loss_val =  0.1648071\n","step =  300 , loss_val =  0.10738509\n","step =  400 , loss_val =  0.32635358\n","step =  500 , loss_val =  0.21429469\n","\n","Accuracy =  0.9212\n","step =  0 , loss_val =  0.3580061\n","step =  100 , loss_val =  0.15242435\n","step =  200 , loss_val =  0.14736007\n","step =  300 , loss_val =  0.15564229\n","step =  400 , loss_val =  0.52776086\n","step =  500 , loss_val =  0.21309265\n","\n","Accuracy =  0.9252\n","step =  0 , loss_val =  0.2031989\n","step =  100 , loss_val =  0.15522629\n","step =  200 , loss_val =  0.21078575\n","step =  300 , loss_val =  0.14322338\n","step =  400 , loss_val =  0.36306077\n","step =  500 , loss_val =  0.18097919\n","\n","Accuracy =  0.9273\n","step =  0 , loss_val =  0.23188783\n","step =  100 , loss_val =  0.24232209\n","step =  200 , loss_val =  0.19909063\n","step =  300 , loss_val =  0.19803798\n","step =  400 , loss_val =  0.18145332\n","step =  500 , loss_val =  0.14459918\n","\n","Accuracy =  0.9268\n","step =  0 , loss_val =  0.12465828\n","step =  100 , loss_val =  0.22322847\n","step =  200 , loss_val =  0.25729966\n","step =  300 , loss_val =  0.23552345\n","step =  400 , loss_val =  0.2925318\n","step =  500 , loss_val =  0.1980258\n","\n","Accuracy =  0.9267\n","step =  0 , loss_val =  0.12110842\n","step =  100 , loss_val =  0.22911417\n","step =  200 , loss_val =  0.29530615\n","step =  300 , loss_val =  0.14500715\n","step =  400 , loss_val =  0.32780075\n","step =  500 , loss_val =  0.21283688\n","\n","Accuracy =  0.93\n","step =  0 , loss_val =  0.19492397\n","step =  100 , loss_val =  0.14159282\n","step =  200 , loss_val =  0.110092096\n","step =  300 , loss_val =  0.107640326\n","step =  400 , loss_val =  0.13649012\n","step =  500 , loss_val =  0.17378576\n","\n","Accuracy =  0.9268\n","step =  0 , loss_val =  0.31852236\n","step =  100 , loss_val =  0.29003698\n","step =  200 , loss_val =  0.1538818\n","step =  300 , loss_val =  0.25455633\n","step =  400 , loss_val =  0.12521766\n","step =  500 , loss_val =  0.20298617\n","\n","Accuracy =  0.9264\n","step =  0 , loss_val =  0.1336652\n","step =  100 , loss_val =  0.112280264\n","step =  200 , loss_val =  0.20087835\n","step =  300 , loss_val =  0.11568404\n","step =  400 , loss_val =  0.15964104\n","step =  500 , loss_val =  0.25411737\n","\n","Accuracy =  0.9267\n","step =  0 , loss_val =  0.22812879\n","step =  100 , loss_val =  0.16337089\n","step =  200 , loss_val =  0.2146307\n","step =  300 , loss_val =  0.18416752\n","step =  400 , loss_val =  0.100868806\n","step =  500 , loss_val =  0.28707594\n","\n","Accuracy =  0.9272\n","step =  0 , loss_val =  0.18631375\n","step =  100 , loss_val =  0.23947695\n","step =  200 , loss_val =  0.13752347\n","step =  300 , loss_val =  0.12532955\n","step =  400 , loss_val =  0.4982913\n","step =  500 , loss_val =  0.29538637\n","\n","Accuracy =  0.9312\n","step =  0 , loss_val =  0.25559807\n","step =  100 , loss_val =  0.3253667\n","step =  200 , loss_val =  0.18955782\n","step =  300 , loss_val =  0.15841833\n","step =  400 , loss_val =  0.10818115\n","step =  500 , loss_val =  0.23845047\n","\n","Accuracy =  0.9305\n","step =  0 , loss_val =  0.16399159\n","step =  100 , loss_val =  0.1416403\n","step =  200 , loss_val =  0.24676904\n","step =  300 , loss_val =  0.17998002\n","step =  400 , loss_val =  0.15145357\n","step =  500 , loss_val =  0.06087961\n","\n","Accuracy =  0.93\n","step =  0 , loss_val =  0.09689696\n","step =  100 , loss_val =  0.18397275\n","step =  200 , loss_val =  0.14317875\n","step =  300 , loss_val =  0.07620499\n","step =  400 , loss_val =  0.21071579\n","step =  500 , loss_val =  0.2170791\n","\n","Accuracy =  0.9304\n","step =  0 , loss_val =  0.11224082\n","step =  100 , loss_val =  0.15474682\n","step =  200 , loss_val =  0.16272451\n","step =  300 , loss_val =  0.1731199\n","step =  400 , loss_val =  0.3231303\n","step =  500 , loss_val =  0.10667052\n","\n","Accuracy =  0.9334\n","step =  0 , loss_val =  0.075100005\n","step =  100 , loss_val =  0.19819456\n","step =  200 , loss_val =  0.15817529\n","step =  300 , loss_val =  0.07980576\n","step =  400 , loss_val =  0.1440298\n","step =  500 , loss_val =  0.33090824\n","\n","Accuracy =  0.9342\n","step =  0 , loss_val =  0.19014572\n","step =  100 , loss_val =  0.23167044\n","step =  200 , loss_val =  0.16724071\n","step =  300 , loss_val =  0.20337127\n","step =  400 , loss_val =  0.17717656\n","step =  500 , loss_val =  0.059309777\n","\n","Accuracy =  0.9349\n","step =  0 , loss_val =  0.16086195\n","step =  100 , loss_val =  0.13355188\n","step =  200 , loss_val =  0.15223457\n","step =  300 , loss_val =  0.20290132\n","step =  400 , loss_val =  0.1311983\n","step =  500 , loss_val =  0.25251377\n","\n","Accuracy =  0.9365\n","step =  0 , loss_val =  0.24454921\n","step =  100 , loss_val =  0.040383495\n","step =  200 , loss_val =  0.13336025\n","step =  300 , loss_val =  0.17324239\n","step =  400 , loss_val =  0.11518526\n","step =  500 , loss_val =  0.25135133\n","\n","Accuracy =  0.9359\n","step =  0 , loss_val =  0.090843104\n","step =  100 , loss_val =  0.12807575\n","step =  200 , loss_val =  0.30835366\n","step =  300 , loss_val =  0.3367399\n","step =  400 , loss_val =  0.26310623\n","step =  500 , loss_val =  0.07880367\n","\n","Accuracy =  0.9377\n","step =  0 , loss_val =  0.12757936\n","step =  100 , loss_val =  0.18921123\n","step =  200 , loss_val =  0.20545907\n","step =  300 , loss_val =  0.23980865\n","step =  400 , loss_val =  0.13432147\n","step =  500 , loss_val =  0.14868236\n","\n","Accuracy =  0.9374\n","step =  0 , loss_val =  0.11164011\n","step =  100 , loss_val =  0.14778036\n","step =  200 , loss_val =  0.22276394\n","step =  300 , loss_val =  0.15349442\n","step =  400 , loss_val =  0.23103045\n","step =  500 , loss_val =  0.2196764\n","\n","Accuracy =  0.9382\n","step =  0 , loss_val =  0.2005804\n","step =  100 , loss_val =  0.2319641\n","step =  200 , loss_val =  0.1032835\n","step =  300 , loss_val =  0.09765515\n","step =  400 , loss_val =  0.08729478\n","step =  500 , loss_val =  0.15806264\n","\n","Accuracy =  0.9346\n","step =  0 , loss_val =  0.10237795\n","step =  100 , loss_val =  0.07039884\n","step =  200 , loss_val =  0.09273383\n","step =  300 , loss_val =  0.17828158\n","step =  400 , loss_val =  0.27590746\n","step =  500 , loss_val =  0.109223954\n","\n","Accuracy =  0.9375\n","step =  0 , loss_val =  0.20729294\n","step =  100 , loss_val =  0.21222967\n","step =  200 , loss_val =  0.15746598\n","step =  300 , loss_val =  0.14845195\n","step =  400 , loss_val =  0.13001522\n","step =  500 , loss_val =  0.27553177\n","\n","Accuracy =  0.9395\n","step =  0 , loss_val =  0.06528244\n","step =  100 , loss_val =  0.11845742\n","step =  200 , loss_val =  0.094393685\n","step =  300 , loss_val =  0.09294508\n","step =  400 , loss_val =  0.03881035\n","step =  500 , loss_val =  0.14505409\n","\n","Accuracy =  0.9393\n","step =  0 , loss_val =  0.066630214\n","step =  100 , loss_val =  0.22938885\n","step =  200 , loss_val =  0.1918967\n","step =  300 , loss_val =  0.14908299\n","step =  400 , loss_val =  0.10897091\n","step =  500 , loss_val =  0.06690794\n","\n","Accuracy =  0.938\n","step =  0 , loss_val =  0.09546555\n","step =  100 , loss_val =  0.18376076\n","step =  200 , loss_val =  0.18811283\n","step =  300 , loss_val =  0.1104802\n","step =  400 , loss_val =  0.08149198\n","step =  500 , loss_val =  0.16226889\n","\n","Accuracy =  0.939\n","step =  0 , loss_val =  0.16654572\n","step =  100 , loss_val =  0.079853244\n","step =  200 , loss_val =  0.22184381\n","step =  300 , loss_val =  0.1208575\n","step =  400 , loss_val =  0.1621911\n","step =  500 , loss_val =  0.29890797\n","\n","Accuracy =  0.9389\n","step =  0 , loss_val =  0.04164626\n","step =  100 , loss_val =  0.17770357\n","step =  200 , loss_val =  0.066647716\n","step =  300 , loss_val =  0.07246336\n","step =  400 , loss_val =  0.16454607\n","step =  500 , loss_val =  0.03373201\n","\n","Accuracy =  0.9403\n","step =  0 , loss_val =  0.20774998\n","step =  100 , loss_val =  0.14690706\n","step =  200 , loss_val =  0.15224254\n","step =  300 , loss_val =  0.16262323\n","step =  400 , loss_val =  0.061653357\n","step =  500 , loss_val =  0.12455225\n","\n","Accuracy =  0.9392\n","step =  0 , loss_val =  0.09297152\n","step =  100 , loss_val =  0.10062998\n","step =  200 , loss_val =  0.035467252\n","step =  300 , loss_val =  0.19199364\n","step =  400 , loss_val =  0.1715316\n","step =  500 , loss_val =  0.1707695\n","\n","Accuracy =  0.9379\n","step =  0 , loss_val =  0.09015673\n","step =  100 , loss_val =  0.15848725\n","step =  200 , loss_val =  0.10781723\n","step =  300 , loss_val =  0.11319525\n","step =  400 , loss_val =  0.2708448\n","step =  500 , loss_val =  0.036609244\n","\n","Accuracy =  0.9404\n","step =  0 , loss_val =  0.16213265\n","step =  100 , loss_val =  0.33137318\n","step =  200 , loss_val =  0.14809345\n","step =  300 , loss_val =  0.07012846\n","step =  400 , loss_val =  0.1587559\n","step =  500 , loss_val =  0.0781331\n","\n","Accuracy =  0.9414\n","step =  0 , loss_val =  0.2267648\n","step =  100 , loss_val =  0.10478064\n","step =  200 , loss_val =  0.07500162\n","step =  300 , loss_val =  0.04940938\n","step =  400 , loss_val =  0.1304915\n","step =  500 , loss_val =  0.24214739\n","\n","Accuracy =  0.9437\n","step =  0 , loss_val =  0.12567022\n","step =  100 , loss_val =  0.10814845\n","step =  200 , loss_val =  0.1394627\n","step =  300 , loss_val =  0.16751307\n","step =  400 , loss_val =  0.20024534\n","step =  500 , loss_val =  0.12269599\n","\n","Accuracy =  0.9414\n","step =  0 , loss_val =  0.21117154\n","step =  100 , loss_val =  0.06686826\n","step =  200 , loss_val =  0.075860575\n","step =  300 , loss_val =  0.24260284\n","step =  400 , loss_val =  0.10461266\n","step =  500 , loss_val =  0.11657801\n","\n","Accuracy =  0.9419\n","step =  0 , loss_val =  0.13341993\n","step =  100 , loss_val =  0.11381142\n","step =  200 , loss_val =  0.13069972\n","step =  300 , loss_val =  0.16495852\n","step =  400 , loss_val =  0.18074971\n","step =  500 , loss_val =  0.1111812\n","\n","Accuracy =  0.9398\n","step =  0 , loss_val =  0.17325635\n","step =  100 , loss_val =  0.16099165\n","step =  200 , loss_val =  0.053648695\n","step =  300 , loss_val =  0.17963006\n","step =  400 , loss_val =  0.06723817\n","step =  500 , loss_val =  0.14015894\n","\n","Accuracy =  0.9414\n","step =  0 , loss_val =  0.11324083\n","step =  100 , loss_val =  0.067071594\n","step =  200 , loss_val =  0.086434364\n","step =  300 , loss_val =  0.062365137\n","step =  400 , loss_val =  0.09665775\n","step =  500 , loss_val =  0.16809626\n","\n","Accuracy =  0.9431\n","step =  0 , loss_val =  0.18287054\n","step =  100 , loss_val =  0.044335403\n","step =  200 , loss_val =  0.11893983\n","step =  300 , loss_val =  0.16799454\n","step =  400 , loss_val =  0.10046372\n","step =  500 , loss_val =  0.27696362\n","\n","Accuracy =  0.9436\n","step =  0 , loss_val =  0.16583757\n","step =  100 , loss_val =  0.056445036\n","step =  200 , loss_val =  0.13000548\n","step =  300 , loss_val =  0.1084542\n","step =  400 , loss_val =  0.062439267\n","step =  500 , loss_val =  0.17496607\n","\n","Accuracy =  0.9435\n","step =  0 , loss_val =  0.19088209\n","step =  100 , loss_val =  0.06478378\n","step =  200 , loss_val =  0.07039926\n","step =  300 , loss_val =  0.053740315\n","step =  400 , loss_val =  0.08160362\n","step =  500 , loss_val =  0.10643461\n","\n","Accuracy =  0.9455\n","step =  0 , loss_val =  0.06537536\n","step =  100 , loss_val =  0.31726238\n","step =  200 , loss_val =  0.124914646\n","step =  300 , loss_val =  0.100297906\n","step =  400 , loss_val =  0.0895467\n","step =  500 , loss_val =  0.2149591\n","\n","Accuracy =  0.9452\n","step =  0 , loss_val =  0.14358793\n","step =  100 , loss_val =  0.125239\n","step =  200 , loss_val =  0.18499327\n","step =  300 , loss_val =  0.031958275\n","step =  400 , loss_val =  0.076984495\n","step =  500 , loss_val =  0.21939774\n","\n","Accuracy =  0.942\n","step =  0 , loss_val =  0.10986543\n","step =  100 , loss_val =  0.08113945\n","step =  200 , loss_val =  0.07615346\n","step =  300 , loss_val =  0.12506992\n","step =  400 , loss_val =  0.094985254\n","step =  500 , loss_val =  0.057116438\n","\n","Accuracy =  0.9436\n","step =  0 , loss_val =  0.19504446\n","step =  100 , loss_val =  0.17417058\n","step =  200 , loss_val =  0.19666508\n","step =  300 , loss_val =  0.051866908\n","step =  400 , loss_val =  0.2735889\n","step =  500 , loss_val =  0.16387205\n","\n","Accuracy =  0.9457\n","step =  0 , loss_val =  0.11560172\n","step =  100 , loss_val =  0.10620856\n","step =  200 , loss_val =  0.07697543\n","step =  300 , loss_val =  0.14716023\n","step =  400 , loss_val =  0.11140567\n","step =  500 , loss_val =  0.08243134\n","\n","Accuracy =  0.9429\n","step =  0 , loss_val =  0.10374886\n","step =  100 , loss_val =  0.061369162\n","step =  200 , loss_val =  0.3522149\n","step =  300 , loss_val =  0.2640304\n","step =  400 , loss_val =  0.08791521\n","step =  500 , loss_val =  0.21992438\n","\n","Accuracy =  0.9433\n","step =  0 , loss_val =  0.056786794\n","step =  100 , loss_val =  0.13371502\n","step =  200 , loss_val =  0.19274399\n","step =  300 , loss_val =  0.1600454\n","step =  400 , loss_val =  0.13584109\n","step =  500 , loss_val =  0.22398706\n","\n","Accuracy =  0.9469\n","step =  0 , loss_val =  0.06996368\n","step =  100 , loss_val =  0.11770242\n","step =  200 , loss_val =  0.0760352\n","step =  300 , loss_val =  0.08649922\n","step =  400 , loss_val =  0.05344197\n","step =  500 , loss_val =  0.0893147\n","\n","Accuracy =  0.946\n","step =  0 , loss_val =  0.04319107\n","step =  100 , loss_val =  0.37950626\n","step =  200 , loss_val =  0.08551243\n","step =  300 , loss_val =  0.07704136\n","step =  400 , loss_val =  0.08034075\n","step =  500 , loss_val =  0.10608859\n","\n","Accuracy =  0.947\n","step =  0 , loss_val =  0.075147025\n","step =  100 , loss_val =  0.12894687\n","step =  200 , loss_val =  0.0731472\n","step =  300 , loss_val =  0.06821078\n","step =  400 , loss_val =  0.05047755\n","step =  500 , loss_val =  0.07686147\n","\n","Accuracy =  0.9438\n","step =  0 , loss_val =  0.18613869\n","step =  100 , loss_val =  0.22138314\n","step =  200 , loss_val =  0.25017023\n","step =  300 , loss_val =  0.18852675\n","step =  400 , loss_val =  0.05430247\n","step =  500 , loss_val =  0.15604463\n","\n","Accuracy =  0.9475\n","step =  0 , loss_val =  0.13658528\n","step =  100 , loss_val =  0.09105256\n","step =  200 , loss_val =  0.09980882\n","step =  300 , loss_val =  0.1658225\n","step =  400 , loss_val =  0.105582096\n","step =  500 , loss_val =  0.1775356\n","\n","Accuracy =  0.9464\n","step =  0 , loss_val =  0.17676388\n","step =  100 , loss_val =  0.08093115\n","step =  200 , loss_val =  0.05070288\n","step =  300 , loss_val =  0.077685334\n","step =  400 , loss_val =  0.06187652\n","step =  500 , loss_val =  0.12803544\n","\n","Accuracy =  0.9473\n","step =  0 , loss_val =  0.17051712\n","step =  100 , loss_val =  0.061564717\n","step =  200 , loss_val =  0.21917574\n","step =  300 , loss_val =  0.115674354\n","step =  400 , loss_val =  0.05840466\n","step =  500 , loss_val =  0.1505892\n","\n","Accuracy =  0.9464\n","step =  0 , loss_val =  0.042268563\n","step =  100 , loss_val =  0.15884528\n","step =  200 , loss_val =  0.14400958\n","step =  300 , loss_val =  0.037319504\n","step =  400 , loss_val =  0.31152585\n","step =  500 , loss_val =  0.14683065\n","\n","Accuracy =  0.9468\n","step =  0 , loss_val =  0.14844136\n","step =  100 , loss_val =  0.10287756\n","step =  200 , loss_val =  0.28813842\n","step =  300 , loss_val =  0.21040985\n","step =  400 , loss_val =  0.059151445\n","step =  500 , loss_val =  0.08514936\n","\n","Accuracy =  0.9456\n","step =  0 , loss_val =  0.06932701\n","step =  100 , loss_val =  0.061560735\n","step =  200 , loss_val =  0.095009975\n","step =  300 , loss_val =  0.11564543\n","step =  400 , loss_val =  0.14393428\n","step =  500 , loss_val =  0.14215024\n","\n","Accuracy =  0.9488\n","step =  0 , loss_val =  0.10799757\n","step =  100 , loss_val =  0.065640576\n","step =  200 , loss_val =  0.19004637\n","step =  300 , loss_val =  0.09095481\n","step =  400 , loss_val =  0.13695182\n","step =  500 , loss_val =  0.11056032\n","\n","Accuracy =  0.9483\n","step =  0 , loss_val =  0.099730395\n","step =  100 , loss_val =  0.09864004\n","step =  200 , loss_val =  0.101585776\n","step =  300 , loss_val =  0.08095394\n","step =  400 , loss_val =  0.17524692\n","step =  500 , loss_val =  0.07246086\n","\n","Accuracy =  0.9477\n","step =  0 , loss_val =  0.049964264\n","step =  100 , loss_val =  0.05406216\n","step =  200 , loss_val =  0.087016776\n","step =  300 , loss_val =  0.107307695\n","step =  400 , loss_val =  0.110924385\n","step =  500 , loss_val =  0.03412138\n","\n","Accuracy =  0.949\n","step =  0 , loss_val =  0.091500826\n","step =  100 , loss_val =  0.04474344\n","step =  200 , loss_val =  0.15116245\n","step =  300 , loss_val =  0.10499671\n","step =  400 , loss_val =  0.12988701\n","step =  500 , loss_val =  0.08056963\n","\n","Accuracy =  0.9484\n","step =  0 , loss_val =  0.057423234\n","step =  100 , loss_val =  0.1201461\n","step =  200 , loss_val =  0.024520926\n","step =  300 , loss_val =  0.06500318\n","step =  400 , loss_val =  0.105646156\n","step =  500 , loss_val =  0.06382644\n","\n","Accuracy =  0.9452\n","step =  0 , loss_val =  0.13000895\n","step =  100 , loss_val =  0.085829675\n","step =  200 , loss_val =  0.057167944\n","step =  300 , loss_val =  0.1423347\n","step =  400 , loss_val =  0.20267534\n","step =  500 , loss_val =  0.041162185\n","\n","Accuracy =  0.9471\n","step =  0 , loss_val =  0.010902801\n","step =  100 , loss_val =  0.10564762\n","step =  200 , loss_val =  0.07148987\n","step =  300 , loss_val =  0.115778275\n","step =  400 , loss_val =  0.055573236\n","step =  500 , loss_val =  0.093803905\n","\n","Accuracy =  0.9491\n","step =  0 , loss_val =  0.15260285\n","step =  100 , loss_val =  0.07295087\n","step =  200 , loss_val =  0.06834722\n","step =  300 , loss_val =  0.035118252\n","step =  400 , loss_val =  0.08679262\n","step =  500 , loss_val =  0.06844749\n","\n","Accuracy =  0.9461\n","step =  0 , loss_val =  0.090806246\n","step =  100 , loss_val =  0.047938667\n","step =  200 , loss_val =  0.06745534\n","step =  300 , loss_val =  0.072948955\n","step =  400 , loss_val =  0.11554978\n","step =  500 , loss_val =  0.095646575\n","\n","Accuracy =  0.95\n","step =  0 , loss_val =  0.11793965\n","step =  100 , loss_val =  0.10706135\n","step =  200 , loss_val =  0.18939221\n","step =  300 , loss_val =  0.09185914\n","step =  400 , loss_val =  0.09336077\n","step =  500 , loss_val =  0.028106665\n","\n","Accuracy =  0.9465\n","step =  0 , loss_val =  0.085849255\n","step =  100 , loss_val =  0.07810637\n","step =  200 , loss_val =  0.020853706\n","step =  300 , loss_val =  0.1205\n","step =  400 , loss_val =  0.16445486\n","step =  500 , loss_val =  0.07632983\n","\n","Accuracy =  0.9479\n","step =  0 , loss_val =  0.11282914\n","step =  100 , loss_val =  0.09092502\n","step =  200 , loss_val =  0.07377472\n","step =  300 , loss_val =  0.15332553\n","step =  400 , loss_val =  0.086981095\n","step =  500 , loss_val =  0.033246506\n","\n","Accuracy =  0.9484\n","step =  0 , loss_val =  0.115326144\n","step =  100 , loss_val =  0.14446649\n","step =  200 , loss_val =  0.1411936\n","step =  300 , loss_val =  0.05649442\n","step =  400 , loss_val =  0.21206972\n","step =  500 , loss_val =  0.03995152\n","\n","Accuracy =  0.9487\n","step =  0 , loss_val =  0.05537415\n","step =  100 , loss_val =  0.059911456\n","step =  200 , loss_val =  0.07398549\n","step =  300 , loss_val =  0.10424484\n","step =  400 , loss_val =  0.09636721\n","step =  500 , loss_val =  0.2378345\n","\n","Accuracy =  0.9487\n","step =  0 , loss_val =  0.021088788\n","step =  100 , loss_val =  0.04915353\n","step =  200 , loss_val =  0.060523354\n","step =  300 , loss_val =  0.034889936\n","step =  400 , loss_val =  0.10416078\n","step =  500 , loss_val =  0.09209129\n","\n","Accuracy =  0.9504\n","step =  0 , loss_val =  0.058875456\n","step =  100 , loss_val =  0.013295699\n","step =  200 , loss_val =  0.09775752\n","step =  300 , loss_val =  0.16363683\n","step =  400 , loss_val =  0.048410796\n","step =  500 , loss_val =  0.05772085\n","\n","Accuracy =  0.9509\n","step =  0 , loss_val =  0.06759764\n","step =  100 , loss_val =  0.044439774\n","step =  200 , loss_val =  0.07914443\n","step =  300 , loss_val =  0.013826843\n","step =  400 , loss_val =  0.05356615\n","step =  500 , loss_val =  0.04511414\n","\n","Accuracy =  0.9494\n","step =  0 , loss_val =  0.05250498\n","step =  100 , loss_val =  0.09729219\n","step =  200 , loss_val =  0.30048642\n","step =  300 , loss_val =  0.053988647\n","step =  400 , loss_val =  0.10364084\n","step =  500 , loss_val =  0.016636709\n","\n","Accuracy =  0.9501\n","step =  0 , loss_val =  0.079358555\n","step =  100 , loss_val =  0.10737183\n","step =  200 , loss_val =  0.07844485\n","step =  300 , loss_val =  0.21619539\n","step =  400 , loss_val =  0.04000308\n","step =  500 , loss_val =  0.031156702\n","\n","Accuracy =  0.951\n","step =  0 , loss_val =  0.0579579\n","step =  100 , loss_val =  0.06635284\n","step =  200 , loss_val =  0.048371386\n","step =  300 , loss_val =  0.118984774\n","step =  400 , loss_val =  0.08218649\n","step =  500 , loss_val =  0.079870164\n","\n","Accuracy =  0.9506\n","step =  0 , loss_val =  0.057981644\n","step =  100 , loss_val =  0.07193121\n","step =  200 , loss_val =  0.03811815\n","step =  300 , loss_val =  0.088065736\n","step =  400 , loss_val =  0.105421096\n","step =  500 , loss_val =  0.057689935\n","\n","Accuracy =  0.9486\n","step =  0 , loss_val =  0.14923517\n","step =  100 , loss_val =  0.18706508\n","step =  200 , loss_val =  0.010605138\n","step =  300 , loss_val =  0.032077584\n","step =  400 , loss_val =  0.21106303\n","step =  500 , loss_val =  0.09802035\n","\n","Accuracy =  0.9484\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"T0ZnpBvPqC-x","executionInfo":{"status":"ok","timestamp":1628057527934,"user_tz":-540,"elapsed":45125,"user":{"displayName":"SeokHwan Yang","photoUrl":"","userId":"04573916107899292925"}},"outputId":"d3b04a4d-ec29-4b2b-a3d4-2df90586e759"},"source":["from datetime import datetime\n","\n","# mnist = input_data.read_data_sets(\"MNIST_data/\", one_hot=True)\n","\n","print(\"\")\n","print(\"train.num = \", mnist.train.num_examples, \", test.num = \", mnist.test.num_examples, \", validation.num = \", mnist.validation.num_examples)\n","\n","# shape 및 type(mnist) 확인\n","print(\"type(mnist) = \", type(mnist), \", type(mnist.train.images) = \", type(mnist.train.images), \", type(mnist.train.labels) = \", type(mnist.train.labels))\n","print(\"\\ntrain image shape = \", np.shape(mnist.train.images))\n","print(\"train label shape = \", np.shape(mnist.train.labels))\n","print(\"test image shape = \", np.shape(mnist.test.images))\n","print(\"test label shape = \", np.shape(mnist.test.labels))\n","print(\"\\ntrain image shape = \", mnist.train.images.shape)\n","print(\"test image shape = \", mnist.test.images.shape)\n","print(\"validation image shape = \", mnist.validation.images.shape)\n","\n","# train data 정규화 및 label 의 one-hot encoding 확인\n","# 정규화 확인\n","print(\"length of mnist.train.images = \", len(mnist.train.images))\n","for index in range(len(mnist.train.images)):\n","\tmin_val = np.min(mnist.train.images[index])\n","\tmax_val = np.max(mnist.train.images[index])\n","\n","\tif min_val < 0.0:\n","\t\tprint(\"min value is \", min_val, \", index = \", index)\n","\t\tbreak\n","\n","\tif max_val > 1.0:\n","\t\tprint(\"max value is \", max_val, \", index = \", index)\n","\t\tbreak\n","\n","print(\"\")\n","print(mnist.train.images[0]) # 정규화 확인을 위한 테스트 출력\n","\n","# one-hot encoding 확인\n","print(\"length of mnist.train.images = \", len(mnist.train.labels))\n","\n","for index in range(len(mnist.train.labels)):\n","\tmin_val = np.min(mnist.train.labels[index])\n","\tmax_val = np.max(mnist.train.labels[index])\n","\tif min_val < 0.0:\n","\t\tprint(\"min value is \", min_val, \", index = \", index)\n","\t\tbreak\n","\tif max_val > 1.0:\n","\t\tprint(\"max value is \", max_val, \", index = \", index)\n","\t\tbreak\n","\n","print(\"\")\n","print(mnist.train.labels[0]) # one-hot encoding 확인을 위한 테스트 출력\n","\n","# Hyper-Parameter 설정\n","# 입력노드, 은닉노드, 출력노드, 학습율, 반복횟수, 배치 개수 등 설정\n","learning_rate = 0.1 # 학습율\n","epochs = 30 # 반복횟수\n","batch_size = 100 # 한번에 입력으로 주어지는 MNIST 개수\n","input_nodes = 784 # 입력노드 개수\n","hidden_nodes = 100 # 은닉노드 개수\n","output_nodes = 10 # 출력노드 개수\n","\n","# 입력과 출력을 위한 플레이스홀더 정의\n","X = tf.placeholder(tf.float32, [None, input_nodes])\n","T = tf.placeholder(tf.float32, [None, output_nodes])\n","\n","# 가중치, 바이어스 정의\n","W2 = tf.Variable(tf.random_normal([input_nodes, hidden_nodes])) # 은닉층 가중치 노드\n","b2 = tf.Variable(tf.random_normal([hidden_nodes])) # 은닉층 바이어스 노드\n","W3 = tf.Variable(tf.random_normal([hidden_nodes, output_nodes])) # 출력층 가중치 노드\n","b3 = tf.Variable(tf.random_normal([output_nodes])) # 출력층 바이어스 노드\n","Z2 = tf.matmul(X, W2) + b2 # 선형회귀 선형회귀 값 Z2\n","A2 = tf.nn.relu(Z2) # 은닉층 출력 값 A2, sigmoid 대신 relu 사용\n","\n","# 출력층 선형회귀 값 Z3, 즉 softmax 에 들어가는 입력 값\n","Z3 = logits = tf.matmul(A2, W3) + b3\n","y = A3 = tf.nn.softmax(Z3)\n","loss = tf.reduce_mean( tf.nn.softmax_cross_entropy_with_logits_v2(logits=Z3, labels=T) )\n","optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n","train = optimizer.minimize(loss)\n","\n","# batch_size X 10 데이터에 대해 argmax 를 통해 행단위로 비교함\n","predicted_val = tf.equal( tf.argmax(A3, 1), tf.argmax(T, 1) )\n","\n","# batch_size X 10 의 True, False 를 1 또는 0 으로 변환\n","accuracy = tf.reduce_mean(tf.cast(predicted_val, dtype=tf.float32))\n","\n","# index list 출력\n","accuracy_index = tf.cast(predicted_val, dtype=tf.float32)\n","\n","# 예측값 처리\n","predicted_list = tf.argmax(A3, 1)\n","\n","# Type Check\n","print('type(predicted_val) = ', type(predicted_val), ', type(accuracy) = ', type(accuracy))\n","print('type(accuracy_index) =', type(accuracy_index), ', type(predicted_list) = ', type(predicted_list))\n","\n","with tf.Session() as sess:\n","\tsess.run(tf.global_variables_initializer()) # 변수 노드(tf.Variable) 초기화\n","\tstart_time = datetime.now()\n","\n","\tfor i in range(epochs): # 50 번 반복수행\n","\t\ttotal_batch = int(mnist.train.num_examples / batch_size) # 55,000 / 100\n","\n","\t\tfor step in range(total_batch):\n","\t\t\tbatch_x_data, batch_t_data = mnist.train.next_batch(batch_size)\n","\t\t\tloss_val, _ = sess.run([loss, train],\n","\t\t\tfeed_dict={X: batch_x_data, T: batch_t_data})\n","\n","\t\t\tif step % 100 == 0:\n","\t\t\t\tprint(\"epochs = \", i, \", step = \", step, \", loss_val = \", loss_val)\n","\n","\tend_time = datetime.now()\n","\n","\tprint(\"\")\n","\tprint(\"Elapsed Time => \", end_time-start_time)\n","\tprint(\"\")\n","\n","\t# Accuracy 확인\n","\ttest_x_data = mnist.test.images # 10000 X 784\n","\ttest_t_data = mnist.test.labels # 10000 X 10\n","\n","\taccuracy_val = sess.run(accuracy, feed_dict={X: test_x_data, T: test_t_data})\n","\tprint('\\ntype(accuracy_val) = ', type(accuracy_val), ', Accuracy = ', accuracy_val)\n","\n","\taccuracy_index_val = sess.run(accuracy_index, feed_dict={X: test_x_data, T: test_t_data})\n","\tpredicted_list_val = sess.run(predicted_list, feed_dict={X: test_x_data})\n","\n","\tprint('========================================================')\n","\tprint('type(accuracy_index_val) = ', type(accuracy_index_val), ', len(accuracy_index_val) = ', len(accuracy_index_val))\n","\tprint('type(predicted_list_val) = ', type(predicted_list_val), ', len(predicted_list_val) = ', len(predicted_list_val))\n","\tprint('')\n","\tprint(accuracy_index_val)\n","\tprint(predicted_list_val)\n","\n","\tfalse_data_index = [ index for index, val \tin enumerate(accuracy_index_val) if val == 0 ]\n","\n","\tprint('len(false_data_index) = ', len(false_data_index))\n","\tprint(false_data_index)\n","\n","\tprint('========================================================')\n","\tprint('W2, W3, b2, b3 after learning')\n","\n","\tprint('=======================================')\n","\tW2_val, W3_val, b2_val, b3_val = sess.run([W2, W3, b2, b3])\n","\tprint('W2_val[1] = ' , W2_val[1])\n","\tprint('W3_val[2] = ' , W3_val[2])\n","\tprint('b2_val[1] = ' , b2_val[1])\n","\tprint('b3_val[2] = ' , b3_val[2])\n","\n","\tprint('=======================================')"],"execution_count":25,"outputs":[{"output_type":"stream","text":["\n","train.num =  55000 , test.num =  10000 , validation.num =  5000\n","type(mnist) =  <class 'tensorflow.contrib.learn.python.learn.datasets.base.Datasets'> , type(mnist.train.images) =  <class 'numpy.ndarray'> , type(mnist.train.labels) =  <class 'numpy.ndarray'>\n","\n","train image shape =  (55000, 784)\n","train label shape =  (55000, 10)\n","test image shape =  (10000, 784)\n","test label shape =  (10000, 10)\n","\n","train image shape =  (55000, 784)\n","test image shape =  (10000, 784)\n","validation image shape =  (5000, 784)\n","length of mnist.train.images =  55000\n","\n","[0.         0.         0.         0.         0.         0.\n"," 0.         0.         0.         0.         0.         0.\n"," 0.         0.         0.         0.         0.         0.\n"," 0.         0.         0.         0.         0.         0.\n"," 0.         0.         0.         0.         0.         0.\n"," 0.         0.         0.         0.         0.         0.\n"," 0.         0.         0.         0.         0.         0.\n"," 0.         0.         0.         0.         0.         0.\n"," 0.         0.         0.         0.         0.         0.\n"," 0.         0.         0.         0.         0.         0.\n"," 0.         0.         0.         0.         0.         0.\n"," 0.         0.         0.         0.         0.         0.\n"," 0.         0.         0.         0.         0.         0.\n"," 0.         0.         0.         0.         0.         0.\n"," 0.         0.         0.         0.         0.         0.\n"," 0.         0.         0.         0.         0.         0.\n"," 0.         0.         0.         0.         0.0509804  0.4784314\n"," 0.7372549  0.02352941 0.         0.         0.         0.\n"," 0.         0.         0.         0.         0.         0.\n"," 0.         0.         0.         0.         0.         0.\n"," 0.         0.         0.         0.         0.         0.\n"," 0.         0.28235295 0.7411765  0.9960785  0.9960785  0.54901963\n"," 0.         0.         0.         0.         0.         0.\n"," 0.         0.         0.         0.         0.         0.\n"," 0.         0.         0.         0.         0.         0.\n"," 0.         0.         0.         0.         0.08235294 0.8117648\n"," 0.9960785  0.9960785  0.85098046 0.12156864 0.         0.\n"," 0.         0.         0.         0.         0.         0.\n"," 0.         0.         0.         0.         0.         0.\n"," 0.         0.         0.         0.         0.         0.\n"," 0.         0.07843138 0.72156864 0.9960785  0.9725491  0.78823537\n"," 0.50980395 0.         0.         0.         0.         0.\n"," 0.         0.         0.         0.         0.         0.\n"," 0.         0.         0.         0.         0.         0.\n"," 0.         0.         0.         0.         0.         0.35686275\n"," 0.9960785  0.9960785  0.81568635 0.02745098 0.50980395 0.\n"," 0.         0.         0.         0.         0.         0.\n"," 0.         0.         0.         0.         0.         0.\n"," 0.         0.         0.         0.         0.         0.\n"," 0.         0.         0.02745098 0.68235296 0.9960785  0.854902\n"," 0.14117648 0.01176471 0.20784315 0.         0.         0.\n"," 0.         0.         0.         0.         0.         0.\n"," 0.         0.         0.         0.         0.         0.\n"," 0.         0.         0.         0.         0.         0.\n"," 0.7137255  0.9960785  0.9960785  0.26666668 0.         0.\n"," 0.         0.         0.         0.         0.         0.\n"," 0.         0.         0.         0.         0.         0.\n"," 0.         0.         0.         0.         0.         0.\n"," 0.         0.         0.         0.5019608  0.9960785  0.9960785\n"," 0.73333335 0.00784314 0.         0.         0.         0.\n"," 0.         0.         0.         0.         0.         0.\n"," 0.         0.         0.         0.         0.         0.\n"," 0.         0.         0.         0.         0.         0.\n"," 0.0509804  0.9490197  0.9960785  0.9960785  0.24313727 0.\n"," 0.         0.         0.         0.         0.         0.\n"," 0.         0.         0.         0.         0.         0.\n"," 0.         0.         0.         0.         0.         0.\n"," 0.         0.         0.         0.         0.05490196 0.9960785\n"," 0.9960785  0.70980394 0.03529412 0.         0.         0.\n"," 0.         0.         0.         0.         0.         0.\n"," 0.         0.         0.         0.         0.         0.\n"," 0.         0.         0.         0.         0.         0.\n"," 0.         0.         0.454902   0.9960785  0.9960785  0.9058824\n"," 0.58431375 0.15686275 0.         0.         0.         0.\n"," 0.         0.         0.         0.         0.         0.\n"," 0.         0.         0.         0.         0.         0.\n"," 0.         0.         0.         0.         0.         0.\n"," 0.59607846 0.9960785  0.9960785  0.9960785  1.         0.9333334\n"," 0.5921569  0.0509804  0.05882353 0.         0.         0.\n"," 0.         0.         0.         0.         0.         0.\n"," 0.         0.         0.         0.         0.         0.\n"," 0.         0.         0.         0.07843138 0.8235295  0.9960785\n"," 0.9960785  0.9725491  0.9725491  0.9921569  0.9960785  0.76470596\n"," 0.78823537 0.04313726 0.         0.         0.         0.\n"," 0.         0.         0.         0.         0.         0.\n"," 0.         0.         0.         0.         0.         0.\n"," 0.         0.13725491 0.9960785  0.9960785  0.5176471  0.05490196\n"," 0.         0.73333335 0.9960785  0.9960785  0.9960785  0.78823537\n"," 0.06666667 0.         0.         0.         0.         0.\n"," 0.         0.         0.         0.         0.         0.\n"," 0.         0.         0.         0.         0.         0.6509804\n"," 0.9960785  0.9960785  0.02352941 0.         0.         0.03921569\n"," 0.45098042 0.9960785  0.9960785  0.9960785  0.13333334 0.\n"," 0.         0.         0.         0.         0.         0.\n"," 0.         0.         0.         0.         0.         0.\n"," 0.         0.         0.         0.20784315 0.9960785  0.9960785\n"," 0.02352941 0.         0.         0.         0.10980393 0.9960785\n"," 0.9960785  0.6431373  0.01960784 0.         0.         0.\n"," 0.         0.         0.         0.         0.         0.\n"," 0.         0.         0.         0.         0.         0.\n"," 0.         0.13725491 0.9960785  0.9960785  0.5529412  0.16078432\n"," 0.06666667 0.24705884 0.32941177 0.9960785  0.9960785  0.46274513\n"," 0.         0.         0.         0.         0.         0.\n"," 0.         0.         0.         0.         0.         0.\n"," 0.         0.         0.         0.         0.         0.04313726\n"," 0.7137255  0.9960785  0.9960785  0.8941177  0.7843138  0.9960785\n"," 0.9960785  0.9921569  0.5686275  0.01568628 0.         0.\n"," 0.         0.         0.         0.         0.         0.\n"," 0.         0.         0.         0.         0.         0.\n"," 0.         0.         0.         0.         0.25490198 0.9176471\n"," 0.9960785  0.9960785  0.9960785  0.9960785  0.98823535 0.89019614\n"," 0.         0.         0.         0.         0.         0.\n"," 0.         0.         0.         0.         0.         0.\n"," 0.         0.         0.         0.         0.         0.\n"," 0.         0.         0.         0.         0.44705886 0.64705884\n"," 0.9960785  0.8941177  0.40784317 0.         0.         0.\n"," 0.         0.         0.         0.         0.         0.\n"," 0.         0.         0.         0.         0.         0.\n"," 0.         0.         0.         0.         0.         0.\n"," 0.         0.         0.         0.         0.         0.\n"," 0.         0.         0.         0.         0.         0.\n"," 0.         0.         0.         0.         0.         0.\n"," 0.         0.         0.         0.         0.         0.\n"," 0.         0.         0.         0.         0.         0.\n"," 0.         0.         0.         0.         0.         0.\n"," 0.         0.         0.         0.         0.         0.\n"," 0.         0.         0.         0.         0.         0.\n"," 0.         0.         0.         0.         0.         0.\n"," 0.         0.         0.         0.         0.         0.\n"," 0.         0.         0.         0.         0.         0.\n"," 0.         0.         0.         0.         0.         0.\n"," 0.         0.         0.         0.         0.         0.\n"," 0.         0.         0.         0.         0.         0.\n"," 0.         0.         0.         0.         0.         0.\n"," 0.         0.         0.         0.         0.         0.\n"," 0.         0.         0.         0.         0.         0.\n"," 0.         0.         0.         0.         0.         0.\n"," 0.         0.         0.         0.         0.         0.\n"," 0.         0.         0.         0.         0.         0.\n"," 0.         0.         0.         0.         0.         0.\n"," 0.         0.         0.         0.        ]\n","length of mnist.train.images =  55000\n","\n","[0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]\n","type(predicted_val) =  <class 'tensorflow.python.framework.ops.Tensor'> , type(accuracy) =  <class 'tensorflow.python.framework.ops.Tensor'>\n","type(accuracy_index) = <class 'tensorflow.python.framework.ops.Tensor'> , type(predicted_list) =  <class 'tensorflow.python.framework.ops.Tensor'>\n","epochs =  0 , step =  0 , loss_val =  89.46709\n","epochs =  0 , step =  100 , loss_val =  5.2419806\n","epochs =  0 , step =  200 , loss_val =  2.7319567\n","epochs =  0 , step =  300 , loss_val =  3.1394613\n","epochs =  0 , step =  400 , loss_val =  0.49153775\n","epochs =  0 , step =  500 , loss_val =  1.381217\n","epochs =  1 , step =  0 , loss_val =  1.128265\n","epochs =  1 , step =  100 , loss_val =  1.1189032\n","epochs =  1 , step =  200 , loss_val =  1.1605046\n","epochs =  1 , step =  300 , loss_val =  1.4923705\n","epochs =  1 , step =  400 , loss_val =  2.042817\n","epochs =  1 , step =  500 , loss_val =  0.9358995\n","epochs =  2 , step =  0 , loss_val =  1.2712761\n","epochs =  2 , step =  100 , loss_val =  0.9522585\n","epochs =  2 , step =  200 , loss_val =  0.9099819\n","epochs =  2 , step =  300 , loss_val =  0.29280367\n","epochs =  2 , step =  400 , loss_val =  0.9307419\n","epochs =  2 , step =  500 , loss_val =  0.6081656\n","epochs =  3 , step =  0 , loss_val =  1.5183425\n","epochs =  3 , step =  100 , loss_val =  1.0573308\n","epochs =  3 , step =  200 , loss_val =  0.40389538\n","epochs =  3 , step =  300 , loss_val =  0.49300003\n","epochs =  3 , step =  400 , loss_val =  0.55633366\n","epochs =  3 , step =  500 , loss_val =  0.641494\n","epochs =  4 , step =  0 , loss_val =  0.4668015\n","epochs =  4 , step =  100 , loss_val =  0.31887886\n","epochs =  4 , step =  200 , loss_val =  0.5777213\n","epochs =  4 , step =  300 , loss_val =  0.44311252\n","epochs =  4 , step =  400 , loss_val =  0.25032538\n","epochs =  4 , step =  500 , loss_val =  0.3585044\n","epochs =  5 , step =  0 , loss_val =  0.72559637\n","epochs =  5 , step =  100 , loss_val =  0.44231698\n","epochs =  5 , step =  200 , loss_val =  0.33188233\n","epochs =  5 , step =  300 , loss_val =  0.6210186\n","epochs =  5 , step =  400 , loss_val =  0.4209157\n","epochs =  5 , step =  500 , loss_val =  0.3674609\n","epochs =  6 , step =  0 , loss_val =  0.1783094\n","epochs =  6 , step =  100 , loss_val =  0.31301895\n","epochs =  6 , step =  200 , loss_val =  0.35175815\n","epochs =  6 , step =  300 , loss_val =  0.3156347\n","epochs =  6 , step =  400 , loss_val =  0.57354116\n","epochs =  6 , step =  500 , loss_val =  0.370877\n","epochs =  7 , step =  0 , loss_val =  0.36016518\n","epochs =  7 , step =  100 , loss_val =  0.2930743\n","epochs =  7 , step =  200 , loss_val =  0.3722657\n","epochs =  7 , step =  300 , loss_val =  0.26326978\n","epochs =  7 , step =  400 , loss_val =  0.2712\n","epochs =  7 , step =  500 , loss_val =  0.28167436\n","epochs =  8 , step =  0 , loss_val =  0.2764129\n","epochs =  8 , step =  100 , loss_val =  0.33932197\n","epochs =  8 , step =  200 , loss_val =  0.35877073\n","epochs =  8 , step =  300 , loss_val =  0.28630725\n","epochs =  8 , step =  400 , loss_val =  0.4408699\n","epochs =  8 , step =  500 , loss_val =  0.1306211\n","epochs =  9 , step =  0 , loss_val =  0.23167762\n","epochs =  9 , step =  100 , loss_val =  0.26769328\n","epochs =  9 , step =  200 , loss_val =  0.42601663\n","epochs =  9 , step =  300 , loss_val =  0.34180072\n","epochs =  9 , step =  400 , loss_val =  0.37648132\n","epochs =  9 , step =  500 , loss_val =  0.14641307\n","epochs =  10 , step =  0 , loss_val =  0.4384817\n","epochs =  10 , step =  100 , loss_val =  0.31208485\n","epochs =  10 , step =  200 , loss_val =  0.3543878\n","epochs =  10 , step =  300 , loss_val =  0.3596484\n","epochs =  10 , step =  400 , loss_val =  0.15517652\n","epochs =  10 , step =  500 , loss_val =  0.20415038\n","epochs =  11 , step =  0 , loss_val =  0.27807206\n","epochs =  11 , step =  100 , loss_val =  0.35586947\n","epochs =  11 , step =  200 , loss_val =  0.2688866\n","epochs =  11 , step =  300 , loss_val =  0.13603659\n","epochs =  11 , step =  400 , loss_val =  0.27273133\n","epochs =  11 , step =  500 , loss_val =  0.37023178\n","epochs =  12 , step =  0 , loss_val =  0.21022\n","epochs =  12 , step =  100 , loss_val =  0.3621133\n","epochs =  12 , step =  200 , loss_val =  0.20301315\n","epochs =  12 , step =  300 , loss_val =  0.23967612\n","epochs =  12 , step =  400 , loss_val =  0.24155727\n","epochs =  12 , step =  500 , loss_val =  0.6830908\n","epochs =  13 , step =  0 , loss_val =  0.41568473\n","epochs =  13 , step =  100 , loss_val =  0.18058069\n","epochs =  13 , step =  200 , loss_val =  0.1469869\n","epochs =  13 , step =  300 , loss_val =  0.15912876\n","epochs =  13 , step =  400 , loss_val =  0.14431459\n","epochs =  13 , step =  500 , loss_val =  0.34021038\n","epochs =  14 , step =  0 , loss_val =  0.25474876\n","epochs =  14 , step =  100 , loss_val =  0.14507179\n","epochs =  14 , step =  200 , loss_val =  0.23122662\n","epochs =  14 , step =  300 , loss_val =  0.12975469\n","epochs =  14 , step =  400 , loss_val =  0.32890034\n","epochs =  14 , step =  500 , loss_val =  0.113854796\n","epochs =  15 , step =  0 , loss_val =  0.14898309\n","epochs =  15 , step =  100 , loss_val =  0.1690748\n","epochs =  15 , step =  200 , loss_val =  0.4898592\n","epochs =  15 , step =  300 , loss_val =  0.1881143\n","epochs =  15 , step =  400 , loss_val =  0.24281913\n","epochs =  15 , step =  500 , loss_val =  0.3423793\n","epochs =  16 , step =  0 , loss_val =  0.14372633\n","epochs =  16 , step =  100 , loss_val =  0.4892107\n","epochs =  16 , step =  200 , loss_val =  0.13726121\n","epochs =  16 , step =  300 , loss_val =  0.1532807\n","epochs =  16 , step =  400 , loss_val =  0.30927056\n","epochs =  16 , step =  500 , loss_val =  0.1967292\n","epochs =  17 , step =  0 , loss_val =  0.08510398\n","epochs =  17 , step =  100 , loss_val =  0.3186773\n","epochs =  17 , step =  200 , loss_val =  0.36890948\n","epochs =  17 , step =  300 , loss_val =  0.1807619\n","epochs =  17 , step =  400 , loss_val =  0.28931534\n","epochs =  17 , step =  500 , loss_val =  0.17544994\n","epochs =  18 , step =  0 , loss_val =  0.07917139\n","epochs =  18 , step =  100 , loss_val =  0.30034173\n","epochs =  18 , step =  200 , loss_val =  0.17770356\n","epochs =  18 , step =  300 , loss_val =  0.12504709\n","epochs =  18 , step =  400 , loss_val =  0.27187526\n","epochs =  18 , step =  500 , loss_val =  0.22618374\n","epochs =  19 , step =  0 , loss_val =  0.27820256\n","epochs =  19 , step =  100 , loss_val =  0.07027574\n","epochs =  19 , step =  200 , loss_val =  0.13999014\n","epochs =  19 , step =  300 , loss_val =  0.7411053\n","epochs =  19 , step =  400 , loss_val =  0.27044246\n","epochs =  19 , step =  500 , loss_val =  0.12849349\n","epochs =  20 , step =  0 , loss_val =  0.32273623\n","epochs =  20 , step =  100 , loss_val =  0.2250517\n","epochs =  20 , step =  200 , loss_val =  0.22959548\n","epochs =  20 , step =  300 , loss_val =  0.19084938\n","epochs =  20 , step =  400 , loss_val =  0.055020217\n","epochs =  20 , step =  500 , loss_val =  0.17028318\n","epochs =  21 , step =  0 , loss_val =  0.2557124\n","epochs =  21 , step =  100 , loss_val =  0.06638428\n","epochs =  21 , step =  200 , loss_val =  0.27609393\n","epochs =  21 , step =  300 , loss_val =  0.10416154\n","epochs =  21 , step =  400 , loss_val =  0.28650054\n","epochs =  21 , step =  500 , loss_val =  0.19771765\n","epochs =  22 , step =  0 , loss_val =  0.112621956\n","epochs =  22 , step =  100 , loss_val =  0.22559927\n","epochs =  22 , step =  200 , loss_val =  0.24422167\n","epochs =  22 , step =  300 , loss_val =  0.14554262\n","epochs =  22 , step =  400 , loss_val =  0.12221312\n","epochs =  22 , step =  500 , loss_val =  0.25123155\n","epochs =  23 , step =  0 , loss_val =  0.13385387\n","epochs =  23 , step =  100 , loss_val =  0.11037135\n","epochs =  23 , step =  200 , loss_val =  0.07612521\n","epochs =  23 , step =  300 , loss_val =  0.39509526\n","epochs =  23 , step =  400 , loss_val =  0.082839906\n","epochs =  23 , step =  500 , loss_val =  0.21792243\n","epochs =  24 , step =  0 , loss_val =  0.30678388\n","epochs =  24 , step =  100 , loss_val =  0.2316112\n","epochs =  24 , step =  200 , loss_val =  0.13967852\n","epochs =  24 , step =  300 , loss_val =  0.16566561\n","epochs =  24 , step =  400 , loss_val =  0.413764\n","epochs =  24 , step =  500 , loss_val =  0.18462996\n","epochs =  25 , step =  0 , loss_val =  0.17443936\n","epochs =  25 , step =  100 , loss_val =  0.10195129\n","epochs =  25 , step =  200 , loss_val =  0.20069724\n","epochs =  25 , step =  300 , loss_val =  0.3284035\n","epochs =  25 , step =  400 , loss_val =  0.3554367\n","epochs =  25 , step =  500 , loss_val =  0.2599319\n","epochs =  26 , step =  0 , loss_val =  0.040374257\n","epochs =  26 , step =  100 , loss_val =  0.15965416\n","epochs =  26 , step =  200 , loss_val =  0.28569278\n","epochs =  26 , step =  300 , loss_val =  0.1758976\n","epochs =  26 , step =  400 , loss_val =  0.22122581\n","epochs =  26 , step =  500 , loss_val =  0.14002584\n","epochs =  27 , step =  0 , loss_val =  0.285155\n","epochs =  27 , step =  100 , loss_val =  0.1206745\n","epochs =  27 , step =  200 , loss_val =  0.1689808\n","epochs =  27 , step =  300 , loss_val =  0.20465767\n","epochs =  27 , step =  400 , loss_val =  0.25887707\n","epochs =  27 , step =  500 , loss_val =  0.24337412\n","epochs =  28 , step =  0 , loss_val =  0.21755943\n","epochs =  28 , step =  100 , loss_val =  0.25949043\n","epochs =  28 , step =  200 , loss_val =  0.31236404\n","epochs =  28 , step =  300 , loss_val =  0.1825037\n","epochs =  28 , step =  400 , loss_val =  0.14235538\n","epochs =  28 , step =  500 , loss_val =  0.15672255\n","epochs =  29 , step =  0 , loss_val =  0.20012942\n","epochs =  29 , step =  100 , loss_val =  0.06299019\n","epochs =  29 , step =  200 , loss_val =  0.14518407\n","epochs =  29 , step =  300 , loss_val =  0.081036285\n","epochs =  29 , step =  400 , loss_val =  0.2886179\n","epochs =  29 , step =  500 , loss_val =  0.084190086\n","\n","Elapsed Time =>  0:00:42.582897\n","\n","\n","type(accuracy_val) =  <class 'numpy.float32'> , Accuracy =  0.9342\n","========================================================\n","type(accuracy_index_val) =  <class 'numpy.ndarray'> , len(accuracy_index_val) =  10000\n","type(predicted_list_val) =  <class 'numpy.ndarray'> , len(predicted_list_val) =  10000\n","\n","[1. 1. 1. ... 1. 1. 1.]\n","[7 2 1 ... 4 5 6]\n","len(false_data_index) =  658\n","[33, 43, 77, 144, 149, 151, 195, 233, 245, 247, 257, 259, 320, 321, 333, 340, 341, 352, 358, 362, 366, 406, 414, 444, 445, 448, 456, 495, 507, 530, 543, 547, 551, 563, 578, 583, 591, 593, 597, 606, 610, 614, 619, 628, 629, 659, 673, 684, 691, 694, 699, 720, 740, 741, 771, 786, 795, 813, 830, 839, 842, 844, 846, 866, 882, 900, 924, 939, 947, 950, 951, 956, 959, 965, 1002, 1003, 1014, 1039, 1044, 1050, 1055, 1062, 1068, 1073, 1082, 1101, 1107, 1112, 1114, 1119, 1128, 1138, 1159, 1164, 1173, 1178, 1181, 1182, 1192, 1194, 1198, 1216, 1224, 1226, 1232, 1247, 1248, 1253, 1256, 1260, 1283, 1287, 1289, 1290, 1299, 1319, 1326, 1328, 1337, 1364, 1414, 1429, 1433, 1439, 1440, 1444, 1464, 1466, 1468, 1469, 1494, 1496, 1500, 1522, 1530, 1549, 1553, 1559, 1569, 1581, 1601, 1607, 1609, 1611, 1621, 1626, 1641, 1678, 1681, 1686, 1689, 1694, 1709, 1717, 1718, 1751, 1754, 1759, 1765, 1772, 1775, 1790, 1800, 1832, 1839, 1847, 1849, 1855, 1856, 1857, 1865, 1874, 1878, 1883, 1901, 1910, 1911, 1930, 1938, 1952, 1954, 1963, 1970, 1984, 2001, 2016, 2018, 2024, 2033, 2035, 2043, 2044, 2053, 2056, 2068, 2093, 2098, 2107, 2109, 2110, 2118, 2121, 2125, 2130, 2135, 2138, 2148, 2174, 2179, 2182, 2185, 2186, 2189, 2191, 2224, 2266, 2268, 2272, 2293, 2299, 2325, 2327, 2339, 2341, 2351, 2358, 2387, 2394, 2404, 2406, 2414, 2422, 2429, 2433, 2447, 2454, 2521, 2526, 2528, 2532, 2548, 2559, 2560, 2597, 2598, 2607, 2610, 2611, 2630, 2636, 2648, 2654, 2665, 2694, 2720, 2721, 2769, 2780, 2797, 2810, 2812, 2823, 2832, 2896, 2915, 2919, 2927, 2936, 2953, 2970, 2979, 3004, 3030, 3060, 3073, 3090, 3100, 3102, 3109, 3110, 3114, 3117, 3136, 3139, 3145, 3157, 3160, 3167, 3189, 3206, 3225, 3239, 3240, 3246, 3250, 3269, 3280, 3283, 3288, 3316, 3330, 3333, 3344, 3369, 3375, 3394, 3412, 3422, 3432, 3437, 3474, 3475, 3503, 3520, 3544, 3549, 3558, 3559, 3564, 3565, 3567, 3573, 3597, 3598, 3654, 3664, 3681, 3688, 3696, 3698, 3702, 3716, 3726, 3751, 3755, 3756, 3757, 3758, 3767, 3780, 3796, 3808, 3817, 3818, 3821, 3833, 3836, 3838, 3846, 3850, 3853, 3855, 3862, 3869, 3893, 3901, 3902, 3906, 3926, 3941, 3943, 3946, 3951, 3968, 3985, 3988, 3994, 4000, 4013, 4065, 4068, 4075, 4078, 4093, 4123, 4131, 4140, 4152, 4156, 4163, 4165, 4176, 4199, 4201, 4205, 4211, 4212, 4224, 4231, 4238, 4248, 4289, 4300, 4315, 4341, 4344, 4350, 4360, 4384, 4400, 4405, 4425, 4433, 4435, 4438, 4443, 4449, 4451, 4463, 4476, 4497, 4498, 4500, 4505, 4515, 4536, 4540, 4545, 4571, 4575, 4578, 4601, 4615, 4639, 4640, 4690, 4699, 4713, 4731, 4735, 4740, 4755, 4761, 4763, 4777, 4788, 4807, 4808, 4814, 4823, 4840, 4852, 4874, 4878, 4880, 4886, 4910, 4918, 4939, 4950, 4966, 4967, 4981, 4990, 5068, 5138, 5140, 5165, 5217, 5268, 5278, 5288, 5299, 5331, 5401, 5562, 5620, 5634, 5642, 5678, 5688, 5709, 5720, 5734, 5735, 5736, 5835, 5857, 5887, 5888, 5913, 5918, 5923, 5955, 5972, 5975, 5985, 6004, 6035, 6037, 6042, 6046, 6065, 6071, 6075, 6091, 6093, 6157, 6166, 6168, 6172, 6173, 6391, 6392, 6400, 6421, 6425, 6505, 6511, 6532, 6542, 6546, 6554, 6555, 6558, 6561, 6564, 6569, 6571, 6574, 6587, 6597, 6598, 6599, 6603, 6610, 6625, 6629, 6650, 6651, 6658, 6706, 6709, 6744, 6746, 6783, 6784, 6847, 6894, 6926, 6940, 7081, 7121, 7200, 7216, 7241, 7249, 7338, 7420, 7432, 7434, 7451, 7472, 7492, 7498, 7529, 7539, 7545, 7552, 7584, 7609, 7619, 7786, 7821, 7849, 7850, 7886, 7899, 7921, 7945, 8020, 8059, 8062, 8072, 8094, 8165, 8183, 8243, 8246, 8255, 8272, 8277, 8278, 8288, 8290, 8326, 8336, 8339, 8502, 8519, 8520, 8523, 8544, 8584, 8624, 8639, 9009, 9010, 9015, 9016, 9019, 9024, 9026, 9031, 9032, 9036, 9045, 9046, 9057, 9211, 9245, 9280, 9303, 9309, 9316, 9426, 9427, 9454, 9482, 9539, 9587, 9634, 9642, 9669, 9695, 9698, 9700, 9716, 9719, 9729, 9732, 9735, 9741, 9744, 9745, 9755, 9762, 9764, 9768, 9770, 9779, 9792, 9808, 9811, 9814, 9832, 9839, 9867, 9874, 9883, 9892, 9893, 9925, 9928, 9944, 9945, 9970, 9991]\n","========================================================\n","W2, W3, b2, b3 after learning\n","=======================================\n","W2_val[1] =  [-1.6532981  -1.2966083   0.26725006  1.1915128  -0.30398172  1.0119584\n","  0.5435461  -0.6083968  -0.72276473  2.1229146   1.2412013   0.73477405\n","  0.9073058  -1.0676566   0.42244476 -0.07036481  0.09679591 -0.1770161\n"," -0.47729534  0.16131507 -1.4618896  -0.1335945   0.7184526  -1.105208\n","  1.6180818   1.2035604  -1.6497245   0.31969503 -0.22770542  0.41683435\n"," -0.13618937  0.07335267  1.1338782   0.5782817  -0.5364742   0.8426767\n","  1.6734442   0.45340693 -0.46148586  0.91004246 -0.36237532  0.59725523\n"," -0.25735533  0.45946854 -1.117146    2.0083694   0.43183786  0.29998702\n","  1.1803142  -0.14527367  0.33270743  1.0459276   0.60621077  1.2509464\n"," -0.4377131   0.59390515 -1.1324183  -0.7387405  -1.313626   -1.2752485\n","  0.69574827  0.23414207 -0.34241164 -0.81034243 -0.08787586 -2.2477448\n","  0.22466822 -1.6329333   0.57726055  0.1792583   0.2634485  -0.40978932\n"," -0.05486349  0.08061789 -1.1306515   1.8932463  -1.1950825   0.7151763\n","  0.01934939 -0.93887544  0.24488476 -0.3332777   0.10902502 -2.457055\n"," -0.03304177 -0.40338975  0.8950375   1.2364138   0.12290203  0.08465821\n"," -2.117038    0.38520893 -0.6035231   0.33843216  0.4567343  -0.86917037\n","  1.3217837  -0.5349578   0.7561923   0.9429734 ]\n","W3_val[2] =  [ 0.1357102  -0.4263795   0.48962173  0.6267067   0.15803863  0.43833935\n","  0.5524795   0.08040729  0.43592218  0.21987188]\n","b2_val[1] =  -0.27023348\n","b3_val[2] =  -1.3569473\n","=======================================\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"de42H8lgybQ6"},"source":[""],"execution_count":null,"outputs":[]}]}